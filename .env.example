# LLM Provider Configuration
# ============================================
# Supported providers: vllm (default), openrouter, ollama, lm-studio
# Default: Local vLLM server with gpt-oss-120b model
LLM_PROVIDER=vllm
LLM_MODEL=gpt-oss-120b
LLM_BASE_URL=http://localhost:8000
# LLM_TIMEOUT=120  # Timeout in seconds (default: 120 for local models)
# LLM_STRIP_THINKING=true  # Strip <think> tags from reasoning model outputs
#
# vLLM Configuration (default - local or SSH-tunneled servers)
# ============================================
# vLLM uses OpenAI-compatible API with reasoning_effort=low for efficiency
# Achieves 100% accuracy with ~2.7s response time per query
#
# For SSH-tunneled vLLM server:
# ssh -N -R 8000:localhost:8000 user@remote-gpu-server
# LLM_BASE_URL=http://localhost:8000
#
# Supported vLLM model families:
# - gpt-oss-*: GPT-OSS reasoning models (default, recommended)
# - Qwen3-Thinking: Reasoning models with <think> tags
# - Qwen3-Instruct: Standard instruct models
# - DeepSeek-R1: DeepSeek reasoning models
#
# OpenRouter Configuration (cloud fallback)
# ============================================
# To use OpenRouter instead of local vLLM:
# LLM_PROVIDER=openrouter
# LLM_MODEL=openai/gpt-4o-mini
# LLM_TIMEOUT=30
#
# Ollama Configuration (for local Ollama models)
# ============================================
# LLM_PROVIDER=ollama
# LLM_BASE_URL=http://localhost:11434
# LLM_MODEL=llama3
#
# LM-Studio Configuration
# ============================================
# LLM_PROVIDER=lm-studio
# LLM_BASE_URL=http://localhost:1234
# LLM_MODEL=local-model

# OpenRouter API Configuration (required if LLM_PROVIDER=openrouter)
# Get your API key from: https://openrouter.ai/keys
OPENROUTER_API_KEY=your_openrouter_api_key_here

# Economic Data API Keys
# FRED API key (optional, but highly recommended for US economic data)
# Get your FRED API key from: https://fred.stlouisfed.org/docs/api/api_key.html
FRED_API_KEY=your_fred_api_key_here

# UN Comtrade API key (optional, but recommended for international trade data)
# Register at: https://comtradedeveloper.un.org/
COMTRADE_API_KEY=your_comtrade_api_key_here

# Supabase Configuration
# Get these from: https://app.supabase.com/project/_/settings/api
SUPABASE_URL=https://your-project-id.supabase.co
SUPABASE_ANON_KEY=your_supabase_anon_key_here
SUPABASE_SERVICE_KEY=your_supabase_service_role_key_here

# Application Configuration
# JWT Secret for authentication (REQUIRED - generate with: openssl rand -hex 32)
# NOTE: When using Supabase, JWT validation uses Supabase's JWT secret
JWT_SECRET=your_jwt_secret_here
# JWT Token expiration in days (default: 7)
JWT_EXPIRES_DAYS=7

# CORS allowed origins (comma-separated list, defaults to localhost for development)
ALLOWED_ORIGINS=http://localhost:5173,http://localhost:3000

# Environment and server configuration
NODE_ENV=development
PORT=3001
FRONTEND_URL=http://localhost:5173

# Feature flags
DISABLE_MCP=false
DISABLE_BACKGROUND_JOBS=false

# Pro Mode Configuration (optional)
# ============================================
# SECURITY: Pro Mode executes AI-generated Python code
# WARNING: Only enable in development or with proper sandboxing (Docker isolation)
# Set to 'false' to completely disable Pro Mode code execution
# Default: false (disabled for security)
PROMODE_ENABLED=false
#
# These directories need to be accessible by both the backend and web server
#
# Public directory for Pro Mode generated files (charts, CSVs, etc.)
# - Development default: {project_root}/public_media/promode
# - Must be served by web server at /static/promode/
# - Production example: /home/hanlulong/OpenEcon/public_media/promode
# PROMODE_PUBLIC_DIR=/home/hanlulong/OpenEcon/public_media/promode
#
# Session storage directory for Pro Mode data persistence
# - Development default: {system_temp}/promode_sessions (e.g., /tmp/promode_sessions)
# - Production example: /tmp/promode_sessions or /var/lib/openecon/sessions
# PROMODE_SESSION_DIR=/tmp/promode_sessions
#
# Pro Mode Implementation Selection
# - Set to 'true' to use LangChain 1.0.5 agent (ReAct pattern with multi-step reasoning)
# - Set to 'false' or leave unset to use Grok-based code generation (default)
# - LangChain provides better success rate (~96%) vs Grok (~87%) but slightly slower
# - Requires langchain dependencies: pip install -r backend/requirements-langchain.txt
# USE_LANGCHAIN_PROMODE=false

# Cache Configuration (in seconds)
CACHE_TTL_DAILY=3600        # 1 hour
CACHE_TTL_MONTHLY=43200     # 12 hours
CACHE_TTL_QUARTERLY=86400   # 24 hours

# Vector Search and Metadata Configuration
# ============================================
# Enable metadata loading on startup (default: false for performance)
ENABLE_METADATA_LOADING=false
#
# Metadata loading timeout in seconds (default: 30)
# Prevents startup from hanging if metadata loading is slow
METADATA_LOADING_TIMEOUT=30
#
# Use FAISS for vector search instead of ChromaDB (default: true)
# FAISS: 100x faster (<100ms load, <5ms search), uses ~200MB memory
# ChromaDB: More features, slower startup (5+ minutes), uses 500MB+ memory
USE_FAISS_INSTEAD_OF_CHROMA=true
#
# Directory to store FAISS index files (default: backend/data/faiss_index)
VECTOR_SEARCH_CACHE_DIR=backend/data/faiss_index

# Rate Limiting
ENABLE_RATE_LIMITING=true

# Logging
LOG_LEVEL=debug
